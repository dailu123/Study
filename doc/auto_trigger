#!/bin/bash

# 1. Define source and destination paths for folders and Git repositories
SOURCE_FOLDER1="/sem_nasi/mca_sem/ui2/smarteda-terminal-service/metadata"
SOURCE_FOLDER2="/sem_nasi/mca_sem/ui2/smarteda-terminal-service/config-cache/EDA-MCA/config-cache"
REPO_FOLDER1="/usr/src/smarteda-terminal/auto-sync/smarteda-terminal/src/main/resources/esc_conf/metadata"
REPO_FOLDER2="/usr/src/smarteda-terminal/auto-sync/smarteda-terminal/src/main/resources/config-cache/EDA-MCA/config-cache"
GIT_REPO_FOLDER="/usr/src/smarteda-terminal/auto-sync/smarteda-terminal-service"

# Define temporary folders: one for source and one for target
SOURCE_TEMP_FOLDER="/sem_nas1/mca_sem/uit2/smarteda/metadata_temp"
TARGET_TEMP_FOLDER="/usr/src/terminal-auto-sync/metadata"

# 2. Prepare source temporary folder and zip the files
echo ">>> mkdir -p \"$SOURCE_TEMP_FOLDER\" <<<"
mkdir -p "$SOURCE_TEMP_FOLDER"

# Zip the source files into the source temporary folder
echo ">>> zip -r \"$SOURCE_TEMP_FOLDER/metadata.zip\" \"$SOURCE_FOLDER1\"/* <<<"
zip -r "$SOURCE_TEMP_FOLDER/metadata.zip" "$SOURCE_FOLDER1"/*

# 3. Prepare target temporary folder and copy the zip file from source to target
echo ">>> mkdir -p \"$TARGET_TEMP_FOLDER\" <<<"
mkdir -p "$TARGET_TEMP_FOLDER"

echo ">>> cp \"$SOURCE_TEMP_FOLDER/metadata.zip\" \"$TARGET_TEMP_FOLDER/metadata.zip\" <<<"
cp "$SOURCE_TEMP_FOLDER/metadata.zip" "$TARGET_TEMP_FOLDER/metadata.zip"

# 4. Unzip the copied file into a subfolder of the target temporary folder
echo ">>> unzip \"$TARGET_TEMP_FOLDER/metadata.zip\" -d \"$TARGET_TEMP_FOLDER/metadata\" <<<"
unzip "$TARGET_TEMP_FOLDER/metadata.zip" -d "$TARGET_TEMP_FOLDER/metadata"

# 5. Remove the zip file after extraction
echo ">>> rm -f \"$TARGET_TEMP_FOLDER/metadata.zip\" <<<"
rm -f "$TARGET_TEMP_FOLDER/metadata.zip"

# 6. Copy unzipped content to the Git repository folders
echo ">>> cp -r \"$TARGET_TEMP_FOLDER/metadata\"/* \"$REPO_FOLDER1\" <<<"
cp -r "$TARGET_TEMP_FOLDER/metadata"/* "$REPO_FOLDER1"

echo ">>> cp -r \"$TARGET_TEMP_FOLDER/metadata\"/* \"$REPO_FOLDER2\" <<<"
cp -r "$TARGET_TEMP_FOLDER/metadata"/* "$REPO_FOLDER2"

# 7. Navigate to the Git repository and switch to the target branch
echo ">>> cd \"$GIT_REPO_FOLDER\" <<<"
cd "$GIT_REPO_FOLDER" || exit

echo ">>> git checkout feature/autosync <<<"
git checkout feature/autosync

# 8. Pull the latest changes from the remote repository
echo ">>> git pull <<<"
git pull

# 9. Stage the changes for commit
echo ">>> git add . <<<"
git add .

# 10. Commit the changes with a timestamped message
COMMIT_MESSAGE="Auto commit config-cache&metadata on $(date +'%Y-%m-%d %H:%M:%S')"
echo ">>> git commit -m \"$COMMIT_MESSAGE\" <<<"
git commit -m "$COMMIT_MESSAGE"

# 11. Push the changes to the remote Git repository
echo ">>> git push <<<"
git push

# 12. Print success message with timestamp
echo ">>> echo \"Files copied and pushed to GitHub successfully at $(date)\" <<<"
echo "Files copied and pushed to GitHub successfully at $(date)"



#!/bin/bash

# 1. Define source folder paths and target Git repository folder paths
SOURCE_FOLDER1="/source/folder1"
SOURCE_FOLDER2="/source/folder2"
REPO_FOLDER1="/path/to/your/repo/folder1"
REPO_FOLDER2="/path/to/your/repo/folder2"

# 2. Copy files from source folders to the target folders in the Git repository
cp -r "$SOURCE_FOLDER1"/* "$REPO_FOLDER1"
cp -r "$SOURCE_FOLDER2"/* "$REPO_FOLDER2"

# 3. Navigate to the Git repository path
cd /path/to/your/repo

# 4. Pull the latest changes from the remote repository to ensure sync
git pull origin main

# 5. Stage all changes in the repository
git add .

# 6. Commit the changes with the current date and time as the commit message
COMMIT_MESSAGE="Auto commit on $(date +"%Y-%m-%d %H:%M:%S")"
git commit -m "$COMMIT_MESSAGE"

# 7. Push the changes to the GitHub repository
git push origin main

# 8. Print completion message with timestamp
echo "Files copied and pushed to GitHub successfully at $(date)"




Event Consumers Architecture Overview
The Event Consumers are responsible for processing events received from the RocketMQ clusters. This component includes several key modules: Event Reception Module, Event Deduplication Module, Event Processing Module, and Reconciliation Module. Below is an overview of each module's function and their interaction within the system.

Modules Overview:
Event Reception Module:

Function: This module establishes connections with all three clusters (primary and backups) to receive messages.
Control Event Handling: Each control event is broadcasted to all clusters. This ensures redundancy and also serves as a heartbeat mechanism for backup clusters. If a consumer does not receive control events from a particular cluster for a specified period (beyond the expected interval), an alert is triggered.
Event Deduplication Module:

Function: Handles the deduplication of events. RocketMQ ensures at least once delivery, which may result in duplicate messages. This module uses a unique event identifier to perform idempotent processing, ensuring that each event is processed only once.
Event Processing Module:

Function: This is where the actual business logic is executed. Messages that pass through the deduplication process are categorized based on type:
Business Messages: Processed according to the business logic defined for these messages.
Control Events: Processed separately to manage and monitor the system's health and message flow.
Reconciliation Module:

Function: Works closely with the Event Processing Module, especially for control events. It ensures data integrity by comparing received control events with expected business event IDs. If any expected events are missing or there are discrepancies, the module triggers appropriate reconciliation actions.
Detailed Module Interaction and Flow
Message Reception:

Messages are received concurrently from all connected clusters by the Event Reception Module.
Control events act as both operational commands and heartbeat signals for monitoring cluster health.
Deduplication Process:

Upon receipt, messages are passed to the Event Deduplication Module, where they are checked against a stored list of processed event IDs.
Only unique events are forwarded for further processing.
Event Processing and Classification:

The deduplicated messages are categorized:
Business Messages: These are forwarded to business-specific processing units.
Control Events: Sent to both control-specific units and the Reconciliation Module.
Reconciliation and Alerting:

The Reconciliation Module uses control events to verify the completeness of the business messages received during a specific time interval.
If discrepancies are found (e.g., missing events), the module logs these and triggers alerts as necessary.
This architecture ensures robust and efficient handling of messages, with a focus on reliability and fault tolerance, particularly important in a multi-cluster environment. The combination of deduplication, processing, and reconciliation processes helps maintain data integrity and operational continuity.








2. Event Bridge
The Event Bridge serves as an intermediary that channels messages from the Event Adapter to the consumers. It ensures that the messages are routed according to the system's routing logic and failover strategies.

3. Event Consumers
Event Consumers are responsible for processing messages received from the Event Bridge. They are designed to connect to multiple RocketMQ clusters, ensuring that they can always retrieve and process data, regardless of individual cluster availability. Consumers handle messages from any available cluster and include mechanisms for deduplication and data integrity.



The "dag_level" object in this JSON represents the configuration for the first level of tasks that will be triggered by our Airflow program, which we call "auto trigger". These tasks will make API requests to a downstream system using the specified HTTP connection ID, HTTP method, endpoint, and data payload. The purpose of this configuration is to specify the details of the API requests that need to be made and the order in which they should be triggered. By defining this configuration in the "dag_level" object, we can easily add, modify, or remove tasks in this level as needed.

---
The file_level object is another section of the configuration file for the "auto trigger" program. It contains multiple file objects, such as file_a, file_b, etc. Each file object can have its own project_id array, which specifies the project or projects that the file belongs to. Additionally, each file object can also have one or more http_cfg objects, which are similar to the DAG level configuration.

Overall, the file_level object allows the "auto trigger" program to manage different files that are related to different projects, and to configure the appropriate API calls for each file based on the http_cfg objects.

Each parameter in the configuration will only take effect if it is configured. Otherwise, the corresponding task will not be loaded. This allows for more flexibility in customizing the configuration to suit specific needs.

---

project_level is another object in the JSON configuration that defines the configuration for each project. It contains an array of projects, where each project is represented as an object with a project name as the key and an array of tasks as the value. Each task in the array represents a configuration for a task that triggers the downstream system's API.

Just like the file_level, each task in the project_level object can have its own configuration for http_cfg, http_conn_id, method, endPoint, and data. Also, the tasks can be configured to belong to one or multiple projects using the project_id parameter.

Similar to the dag_level and file_level, the tasks in project_level will only be added as tasks to the Airflow DAG if they are configured with the relevant parameters.


1.1
based on Apache Airflow. The purpose of this document is to provide a clear understanding of the auto trigger process and design approach for the downstream systems. Specifically, it aims to address the business requirement of propagating CBTDOC data immediately after landing on GCP or Hadoop and initiating the transfer through API trigger.

The document will introduce the three categories of Airflow API trigger approaches, including API trigger in one go country by country, API trigger by file level, and API trigger by project level. The main focus of this document is to provide a detailed design approach for the auto trigger process, which includes auto trigger configuration and auto trigger flow control.

By following the design approach proposed in this document, the downstream users will be able to trigger the API call for the desired data transfer in a timely and efficient manner, thus meeting the business requirement and enhancing the overall data propagation process.

---2.4
The Airflow variable includes a JSON object that controls the API trigger level. The parameters for project level, file level, and DAG level are independent of each other and can be controlled individually.

a) API Trigger by project: If the "project level" JSON object is not null, the API trigger will be by project level. If it is null, the trigger won't be at the project level.

b) API Trigger by file: If the "file level" JSON object is not null, the API trigger will be at the file level. If it is null, the trigger won't be at the file level.

c) API Trigger by DAG: If the "dag level" JSON object is not null, the API trigger will be at the DAG level. If it is null, the trigger won't be at the DAG level.

Event trigger tasks will be generated in Airflow based on the JSON object. The API call related information is configured in Airflow variable and connection, allowing for calling different downstream users based on the configurations.

---

Assumptions and constraints are important factors to consider when designing a system. They help identify potential risks and limitations that may impact the system's functionality or performance. Here are some examples of assumptions and constraints that could be relevant to your design:

Assumptions:

The CBTDOC data will be available on GCP or Hadoop at a specific time.
The downstream users have the necessary access permissions to trigger the API calls.
The API calls will be able to handle large volumes of data and high traffic.
The downstream systems are able to process and consume the data in a timely manner.
Constraints:

The design must comply with existing security and compliance policies.
The system must be able to handle the workload within the allocated resources and infrastructure.
The API trigger process should not impact other data processing or workflows in the system.
The system must be able to handle errors and failures in a resilient and efficient manner.
These are just some examples, and it's important to identify and document all relevant assumptions and constraints for your specific design.


----
Risks and Mitigations:

API Trigger Failure: In the event of an API trigger failure, downstream users will not be able to initiate the transfer of CBTDOC data. This could potentially lead to delays in data propagation and negatively impact business operations.
Mitigation: To mitigate the risk of API trigger failure, a robust monitoring system will be put in place to alert system administrators in the event of any issues. Additionally, regular testing and maintenance of the API trigger process will be conducted to ensure its reliability.

Data Security Breach: The transfer of CBTDOC data may potentially expose sensitive business information to security threats, such as data breaches or unauthorized access.
Mitigation: The API trigger process will be designed with security as a top priority, with encryption and other security measures implemented to protect the transfer of sensitive data. Access controls will also be put in place to ensure that only authorized users have access to the data.

Dependency Management Issues: Dependency management issues may arise as a result of updates or changes to the software components used in the API trigger process. This could potentially cause compatibility issues or other problems that may impact the reliability of the API trigger process.
Mitigation: Regular testing and maintenance of the API trigger process will be conducted to ensure that any potential compatibility issues are identified and addressed in a timely manner. Additionally, a well-documented version control system will be put in place to keep track of changes and ensure that all components are kept up-to-date and compatible.


----
Testing Plan

To ensure the functionality and reliability of the auto trigger process for downstream systems, a comprehensive testing plan will be executed. The testing plan will include the following steps:

Unit Testing: Unit tests will be conducted to ensure that each component of the auto trigger process is working as expected. The unit tests will be automated and will cover all possible scenarios.
Integration Testing: Integration testing will be conducted to ensure that all components of the auto trigger process are working together as expected. The integration tests will include testing the interaction between different components and testing the flow of data.
System Testing: System testing will be conducted to ensure that the auto trigger process is working correctly in a production-like environment. This will involve testing the system under different loads and volumes of data to ensure that it can handle the expected workload.
User Acceptance Testing (UAT): UAT will be conducted to ensure that the auto trigger process meets the business requirements and is user-friendly. This will involve testing the system with a group of end-users to ensure that it is easy to use and that it meets their needs.
Performance Testing: Performance testing will be conducted to ensure that the auto trigger process can handle the expected workload and provide a timely response. This will involve testing the system under different loads and volumes of data to ensure that it meets the performance requirements.
Security Testing: Security testing will be conducted to ensure that the auto trigger process is secure and that sensitive data is protected. This will involve testing the system for vulnerabilities and ensuring that appropriate security measures are in place.
Regression Testing: Regression testing will be conducted to ensure that any changes made to the auto trigger process do not break any existing functionality. This will involve testing the system after any changes or updates have been made.
Overall, the testing plan will ensure that the auto trigger process is thoroughly tested and meets the business requirements, ensuring the reliability and functionality of the downstream systems.



Dear [Recipient],

I am writing to bring to your attention a critical issue that has arisen with our Juniper system in the HK Prod environment. I want to emphasize that this is not a corebanking feed problem, but rather a functionality issue with Juniper.

We have identified that many Juniper write jobs are showing as successful, but are not actually writing the avro into the bucket. This has led to a false sense of success and is a serious issue that needs to be addressed immediately.

I kindly request that you help us investigate and find the root cause of this problem as soon as possible. Your expertise and support in this matter would be greatly appreciated.

Thank you for your attention to this matter.

Best regards,

[Your Name]
