Epic Title: Flink-RocketMQ Integration for Message Forwarding

Feature Title: Message Forwarding from RocketMQ to Another RocketMQ Topic using Flink

Feature Description: This feature aims to enable message forwarding from a source RocketMQ topic to a target RocketMQ topic using Flink. The feature will involve the following tasks:

Configure the RocketMQ source connector to read messages from the source RocketMQ topic.
Develop a Flink pipeline to consume messages from the source RocketMQ topic and transform them as per requirements.
Configure the RocketMQ sink connector to write the transformed messages to the target RocketMQ topic.
Test the integration to ensure reliable message forwarding with no data loss.
Provide documentation and training materials to facilitate the use and maintenance of the integrated system.
Note: This feature is part of the larger Epic titled "Establishing a Flink-RocketMQ Link", which aims to provide a proof of concept for integrating Flink and RocketMQ for various use cases.


In the context of real-time data processing for a financial institution, as a risk analyst, I want to filter and re-structure incoming messages from a RocketMQ topic using Flink, in order to identify and mitigate potential financial risks.

To achieve this, I need to have a system in place that can consume data from the RocketMQ topic, apply certain filters to extract only relevant information, and then re-structure the data in a format that is suitable for my analysis. The system should be able to handle a high volume of messages and provide real-time processing capabilities to ensure that I can detect risks as soon as possible.

By integrating Flink with RocketMQ, I can build a system that meets these requirements. Flink's stream processing capabilities enable me to apply filters and transform data in real-time, while RocketMQ's messaging system ensures that the messages are reliably delivered to the system. This integration will help me to identify potential financial risks as soon as they arise, which will enable me to take proactive measures to mitigate these risks and protect the interests of the financial institution.


Acceptance Criteria Details:

The system should be able to consume messages from the source RocketMQ topic and apply the specified filters to extract relevant information.
The system should be able to re-structure the filtered data into a suitable format for downstream users.
The system should write the restructured data to the target RocketMQ topic reliably with no data loss.
The system should be able to handle high volumes of incoming messages and provide real-time processing capabilities.
Downstream users should be able to use the restructured data for their specific use cases.
Detail Steps:

Set up and configure Flink and RocketMQ to integrate with each other.
Create a Flink job to consume messages from the source RocketMQ topic and apply the specified filters to extract relevant information.
Use Flink to re-structure the filtered data into a suitable format for downstream users.
Configure RocketMQ to write the restructured data to the target RocketMQ topic reliably with no data loss.
Test the system with a high volume of incoming messages to ensure that it can handle the load and provide real-time processing capabilities.
Verify that downstream users can use the restructured data for their specific use cases.
Make any necessary adjustments to the Flink and RocketMQ configurations to optimize performance and reliability.
Deploy the integrated system to production and monitor for any issues or errors.


Send test messages to the source RocketMQ topic with different payloads and headers.
Confirm that the Flink job consumes the test messages from the source RocketMQ topic and applies the specified filters to extract relevant information.
Verify that the filtered data is re-structured into a suitable format for downstream users.
Ensure that the restructured data is written to the target RocketMQ topic reliably with no data loss.
Validate that the restructured data can be used by downstream users for their specific use cases.
Confirm that the system can handle a high volume of incoming messages and provide real-time processing capabilities without any issues.
Check that the system can recover from failures and continue processing messages without data loss.
Verify that the system is secure against potential vulnerabilities.
Measure the system's response time and throughput under different message loads and conditions.
Note: These end-to-end test steps should validate that the integration between Flink and RocketMQ is working as expected and that the restructured data is delivered reliably with no data loss and can be used by downstream users for their specific use cases.


Epic: Connect Flink with MaxComputer to filter messages

Feature: Use Flink to query tables in MaxComputer and filter incoming messages based on the query results.

As a data analyst, I want to use Flink to connect with MaxComputer and query tables to filter incoming messages, in order to extract relevant data for downstream analysis and processing.

Acceptance Criteria:

Flink job is able to connect with MaxComputer and query tables using SQL statements.
Flink job is able to consume messages from the source system and filter them based on the query results.
Filtered messages are delivered to the target system with no data loss.
The system is able to handle a high volume of incoming messages and provide real-time processing capabilities without any issues.
Detail Steps:

Define the source and target systems and the required message format.
Set up Flink and MaxComputer environments and connect them using appropriate libraries and connectors.
Define the SQL queries to be used for filtering incoming messages.
Implement the Flink job to consume messages from the source system and filter them based on the SQL queries.
Test the Flink job with sample messages and verify that the filtering is working as expected.
Ensure that the filtered messages are delivered to the target system with no data loss.
Conduct load testing to ensure that the system can handle the expected message load.
Perform any necessary performance tuning to optimize the system's response time and throughput.
Conduct any additional testing as necessary to ensure that the system meets the requirements of downstream users.
Note: The Epic/Feature, Acceptance Criteria, and Detail Steps should be customized based on the specific requirements of the POC and the integration between Flink and MaxComputer.


In the context of a company that provides real-time messaging services, as a user, I want to be able to filter incoming messages based on a combination of fixed and dynamic filtering criteria, including data from MaxComputer tables, in order to provide relevant data to downstream users in a timely and efficient manner.

The use case involves implementing a Flink job that receives messages from a RocketMQ topic, applies filtering criteria based on both the message content and data from MaxComputer tables, and then forwards the filtered messages to a downstream RocketMQ topic.

The key requirements for this use case are as follows:

The system should be able to handle a high volume of incoming messages and provide real-time processing capabilities.
The filtering criteria should be configurable by users and support both fixed and dynamic values.
The system should be able to query data from MaxComputer tables in real-time and use this data to filter incoming messages.
The system should be able to handle errors and exceptions gracefully and provide adequate logging and monitoring capabilities.
Once the Flink job is implemented and tested, downstream users should be able to receive filtered messages that are relevant to their specific needs and requirements.