1.2 Scope

The scope of this document is to provide a detailed understanding of the auto trigger process and design approach for propagating CBTDOC data immediately after landing on GCP or Hadoop and initiating the transfer through API trigger. This document aims to provide a comprehensive guide to the design and implementation of the auto trigger process in a way that can be easily understood by both technical and non-technical stakeholders.

The document will cover the following topics:

Overview of the auto trigger process and its benefits
Assumptions and constraints
Risk management and mitigation strategies
Error handling procedures
Testing plan
API trigger by country-level configuration
API trigger by file-level configuration
API trigger by project-level configuration
Monitoring and maintenance considerations
The document is intended to serve as a reference for the development team, project managers, and stakeholders to ensure that the auto trigger process is implemented successfully and meets the business requirements. It is also expected that this document will be used as a basis for future enhancements and modifications to the auto trigger process.


1.3 Out of Scope

The following items are considered out of scope for this project:

Development of the downstream systems that receive the data transferred by the auto trigger process.
Implementation of any authentication or authorization mechanisms for accessing the data.
Integration with any third-party tools or services not explicitly mentioned in this document.
Design or implementation of any user interfaces for interacting with the auto trigger process.
Configuration or maintenance of the underlying infrastructure required for the auto trigger process, including but not limited to servers, databases, and networking equipment.
Performance optimization of the auto trigger process beyond what is necessary to meet the specified business requirements.
Development of any additional features or functionality beyond what is specified in this document.
Development of any custom data transformation or processing logic beyond what is necessary to facilitate the data transfer process.


2.1 Product Overview
1 / 1

The product is a data processing system that aims to automate the data propagation process from CBTDOC data sources to downstream systems. The system is based on Apache Airflow and utilizes its workflow management capabilities to orchestrate the data transfer process.

2.2 Product Features

Auto trigger process: The system provides an auto trigger process that can be configured to trigger API calls for data transfer immediately after the data is landed on GCP or Hadoop.
API trigger by country: The system provides the ability to trigger API calls for data transfer by country.
API trigger by file level: The system provides the ability to trigger API calls for data transfer at the file level.
API trigger by project level: The system provides the ability to trigger API calls for data transfer at the project level.
Error handling: The system includes error handling capabilities to ensure that any errors that occur during the data transfer process are logged and addressed promptly.
Testing plan: The system includes a comprehensive testing plan to ensure that the data propagation process is reliable and efficient.
2.3 User Groups

The primary users of the system are data engineers and analysts who are responsible for ensuring that data is propagated from the CBTDOC data sources to downstream systems. The system may also be used by other members of the organization who require access to the data being transferred.


2.2 Solution Diagram 
The solution diagram for the auto trigger process is illustrated below:

sql
Copy code
       +-------------+                  +----------------+                  +----------------------+
       |             |                  |                |                  |                      |
       | CBTDOC Data |                  |     Google     |                  |     Downstream       |
       |   Source    |                  | Cloud Platform |                  |   Systems/Services   |
       |             |                  |                |                  |                      |
       +-------------+                  +----------------+                  +----------------------+
             |                                     |                                      |
             |    Landing on GCP or Hadoop           |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |        API trigger by project level    |
             |                                     |<-------------------------------------+
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |     Data transfer completed          |
             |                                     |<-------------------------------------+
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             |                                     |                                      |
             V                                     V                                      V
The above diagram illustrates the overall flow of the auto trigger process. The CBTDOC data source is landed on GCP or Hadoop, and the downstream systems/services require the data to be propagated immediately. To address this requirement, the Airflow API trigger is used to initiate the data transfer process. The API trigger is implemented at the project level, which allows the downstream users to trigger the data transfer in a timely and efficient manner.

This solution diagram provides a clear understanding of the auto trigger process flow and the role of the API trigger in data propagation.

2.3 Main Procedure

The main procedure of the auto trigger process for propagating CBTDOC data to downstream systems via Airflow API trigger consists of the following steps:

Landing the CBTDOC data on GCP or Hadoop.
Configuring the auto trigger process through the Airflow UI, including the selection of the trigger type, the specification of the trigger parameters, and the setting of the trigger frequency.
Setting up the API connection between Airflow and the downstream systems.
Initiating the API trigger to transfer the CBTDOC data from GCP or Hadoop to the downstream systems.
Monitoring the API trigger process through the Airflow UI and handling any errors that occur during the transfer.
The main goal of the auto trigger process is to ensure that CBTDOC data is propagated to downstream systems in a timely and efficient manner, reducing manual intervention and enhancing the overall data propagation process. The use of Airflow API trigger provides a scalable and reliable way to automate this process, with the ability to trigger data transfers at various levels (country, file, or project) and with different frequency settings.


Sure, here is the content for section 3.1.1 "Connection Configuration":

3.1.1 Connection Configuration

In order to trigger API calls for data transfer, connections to external systems must be established. The connection configuration includes details such as the connection type, host name, port number, authentication credentials, and other relevant parameters.

For this project, the following connection configurations are required:

Connection to the GCP API to initiate data transfer
Connection to the Hadoop system to initiate data transfer
The connection configuration details should be stored securely and centrally in a configuration management system. Access to the configuration information should be strictly controlled and monitored.

The connections should be tested and verified prior to executing the auto trigger process to ensure that the data transfer is initiated successfully.


To configure a connection in Airflow, navigate to Admin->Connections in the Airflow Web UI.

In the "Connections" interface, you can add a new connection.

When adding a new connection, you need to specify the following information:

Connection ID: A unique identifier for the HTTP connection.
Conn Type: The type of the HTTP connection, which in this case is "HTTP".
Host: The hostname or IP address of the downstream system.
Port: The port number of the downstream system.
Schema: The schema of the downstream system, which in this case is "http".
Login: The login credentials for the downstream system, if required.
Password: The password credentials for the downstream system, if required.
Extra: Any additional configuration parameters required for the HTTP connection, such as SSL certificates or headers.


3.1.2 Variable Configuration

The proposed solution requires configuration of the following variables in Airflow:

project_id: The ID of the project that the DAG belongs to.
dag_id: The ID of the DAG.
http_conn_id: The connection ID for the HTTP connection to the downstream system.
These variables can be defined in the Airflow Web UI by navigating to the Admin->Variables page and clicking the "Create" button.

When defining each variable, it is important to provide a clear description and define its type, such as string or integer.

These variables can be used throughout the DAG code to access the values defined in the Airflow Variables, enabling easier configuration and maintenance of the DAGs.

3.1.2 Variable Configuration

The variable configuration defines the HTTP requests to be made to downstream systems when a DAG is triggered. The JSON file format for the variable configuration includes three levels: DAG level, file level, and project level.

In this specific case, we only have a variable defined at the DAG level, which includes the following information:

http_conn_id: the name of the HTTP connection configured in Airflow
method: the HTTP method to be used, in this case "POST"
endPoint: the endpoint to be called
data: the JSON payload to be sent in the request body
This configuration specifies the HTTP request to be sent to trigger a Juniper job in the downstream system. The data field includes the project name, feed name, and event to be triggered.

To configure this variable in Airflow, the JSON file needs to be uploaded to the Airflow variable store. This can be done through the Airflow web UI by navigating to Admin->Variables and adding a new variable with the JSON content. The variable can then be referenced in a DAG using the Variable.get method.


---desc
The purpose of this change order is to initiate the release of the VNM SBV project in coordination with the HUBm release. The main focus of this change is to upgrade the SBV to the latest version. The change will require testing to ensure that all components are functioning as expected and that the new version of SBV is fully integrated with the HUBm release. The change will also involve communication and collaboration with relevant stakeholders to ensure a smooth and successful release.


Execution Steps:

Clone the project from the Git repository.
Open the project in your preferred IDE.
Compile the source code to generate the class files.
Run the project to test its functionality.
If any issues arise during testing, debug the code and repeat steps 3-4.
Once the project is fully functional, create a JAR file for deployment.
Upload the JAR file to the server or cloud platform where it will be hosted.
Ensure that any necessary configurations or environment variables are set up correctly.
Deploy the project to the target environment.
Verify that the project is running correctly in the target environment.
Of course, the specific execution steps may vary depending on the details of your project, but this gives you a general idea of what to include in the description.


This change order is for the VNM SBV project release, which is a Java project aimed to upgrade SBV. After the HUBm release, this change order is necessary to ensure the compatibility of the SBV upgrade. The following are the execution steps for this change order:

Prepare the release package and share it in the designated folder.
Request the support team to deploy the package to SBV.
Ask the support team to restart the SBV service.
Conduct a health check from the application team to ensure the SBV upgrade has been successful.
Notify stakeholders of the upcoming SBV upgrade.
Conduct a pre-release testing to ensure the package is ready for deployment.
Schedule a maintenance window for the SBV upgrade.
Verify the package has been successfully deployed to SBV.
Conduct post-release testing to ensure the upgrade was successful.
Notify stakeholders that the SBV upgrade has been completed.


Hello, my name is Tywin. I'm excited to be here today. I have seven years of experience in the software industry where I've worked on a variety of projects and developed skills in both back-end and front-end development, including Java, Python, and JavaScript. I'm also a quick learner when it comes to new technologies. For example, I used ChatGPT in a front-end demo and found it to be a great time-saver. During a meeting with some senior managers and the HUB team, my boss gave me the opportunity to share my experience using ChatGPT in this demo. I explained how ChatGPT greatly reduced my working time, which surprised many people in attendance.

In my current role, I've had the opportunity to work on a range of projects, including Core Banking Data on Cloud, Core Banking Event Trigger, and Columbus. These projects are all new technology projects for HUB, which mainly focused on AS400 projects before. I acted as a front-runner in these projects.

Typically, I start by doing the POC for new technology projects and then design the function and program. After that, I bring in some contractors to develop and release the first project, and I hand over the document and program to my teammates, who help release the other projects. I provide advice and support for any future issues.

It's also worth mentioning that I have experience in entrepreneurship. In 2018, I had six partners and we had more than 30 employees at its peak. I was mainly responsible for the technical management, leading a team of up to 5 people to support our WeChat and back-end services.


Thank you very much for listening to my introduction and considering my application.


------------
3.3.1 Exception Handling

To ensure the robustness and stability of the system, it is important to have proper exception handling in place. In case of any exceptions, the system should be able to gracefully handle and recover from the error.

The following exceptions are anticipated in the system:

Connection Error: When the HTTP connection is not established properly, an exception will be thrown. The system should retry the connection with an exponential backoff strategy, and send an alert to the administrator if the issue persists.
Invalid Credentials: If the credentials provided for the HTTP connection are invalid, the system should prompt the user to enter the correct credentials.
API Error: If the downstream API returns an error code, the system should log the error, and if required, alert the administrator or the respective team for corrective action.
Invalid Request Payload: If the request payload sent to the downstream API is invalid, the system should log the error and retry the request with the corrected payload. If the error persists, the system should alert the administrator.
To handle these exceptions, the system will have a dedicated error handling module that will catch the exceptions and take appropriate actions. The error handling module will also log the error messages, including the request payload and the response received from the downstream API. The error handling module will also send alerts to the administrator or the respective team in case of any critical errors.

To ensure the system can recover from an exception, the module will have a retry mechanism that will retry the failed requests with an exponential backoff strategy. The retry mechanism will attempt to connect to the downstream API after a certain interval of time, starting with a smaller interval and gradually increasing the interval between each retry attempt. If the maximum retry limit is reached and the error still persists, the system will alert the administrator or the respective team for corrective action.

The error handling module will also have a threshold configuration that will determine the maximum number of errors that can occur within a certain time period. If the number of errors exceeds the threshold, the system will alert the administrator or the respective team for corrective action.

Overall, the proper exception handling mechanism will ensure the stability and reliability of the system, and minimize the chances of system downtime.
